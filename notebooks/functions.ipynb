{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d1d7a7-19c6-485b-b91f-3b70469c6911",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f7df03-33e1-4948-b350-c411d1b95871",
   "metadata": {},
   "source": [
    "## Metadata Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c56aca-1669-4303-93bf-6d166f8c561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_values(str_input, delimiter):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Convert an input such as \"25-50 g\" to a single\n",
    "    integer value that is the average of the numbers\n",
    "    (37.5 in this case).\n",
    "    \n",
    "    Input:\n",
    "        str_input = the string to convert\n",
    "        delimiter = what to split the string by\n",
    "        \n",
    "    Output:\n",
    "        mean_value\n",
    "    \"\"\"\n",
    "    # Split by the delimiter\n",
    "    sep_values = str_input.split(delimiter)\n",
    "    \n",
    "    # Convert each value to an integer\n",
    "    sep_values = [int(val) for val in sep_values]\n",
    "    \n",
    "    # Calculate the mean value\n",
    "    mean_value = mean(sep_values)\n",
    "    \n",
    "    return mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9393fa-29b3-43a5-ae5a-b8f7b88c5928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pd(value):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    - Clean numerical 'X_pd' columns.\n",
    "    - To be used with `apply()`.\n",
    "    \n",
    "    Pipeline:\n",
    "    - Convert to a string for manipulation.\n",
    "    - Strip non-numerical values.\n",
    "    - Calculate averages as required.\n",
    "    - Return an int value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Values which do not need cleaning\n",
    "    try:\n",
    "        return float(value)\n",
    "    \n",
    "    # Values which need cleaning\n",
    "    except:\n",
    "        # Convert value to a string\n",
    "        value = str(value)\n",
    "\n",
    "        ### REMOVE STRINGS ###\n",
    "        # Check if 'week' is in the value\n",
    "        if ' week' in value:\n",
    "            \n",
    "            # Variation #1: per week\n",
    "            if ' per week' in value:\n",
    "                # Remove the ' per week' string\n",
    "                converted = value.replace(\" per week\", \"\").strip()\n",
    "\n",
    "            # Variation #2: for week\n",
    "            elif ' for week' in value:\n",
    "                # Remove the ' for week' string\n",
    "                converted = value.replace(\" for week\", \"\").strip()\n",
    "            \n",
    "            # Check for a dash\n",
    "            if '/' in converted:\n",
    "                result = split_values(converted, \"/\")\n",
    "            else:\n",
    "                result = converted\n",
    "            \n",
    "            # Get the per day value\n",
    "            per_day = round(int(result)/7, 2)\n",
    "\n",
    "            return per_day\n",
    "        \n",
    "        # Check if ' for mounth' is in the value\n",
    "        elif ' for mounth' in value:\n",
    "            \n",
    "            # Remove the ' for mounth'\n",
    "            converted = value.replace(\" for mounth\", \"\")\n",
    "            \n",
    "            # Check for a dash\n",
    "            if '-' in converted:\n",
    "                result = split_values(converted, \"-\")\n",
    "            else:\n",
    "                result = converted\n",
    "            \n",
    "            # Get the per day value\n",
    "            per_day = round(float(result)/30,  2)\n",
    "            \n",
    "            return per_day\n",
    "        \n",
    "        # Check if '/ month' is in the value\n",
    "        elif '/ month' in value:\n",
    "            \n",
    "            # Convert to a list delimited by a space\n",
    "            space_list = value.split(\" \")\n",
    "            \n",
    "            # Get the per day value\n",
    "            per_day = int(space_list[0])/30\n",
    "            \n",
    "            return per_day\n",
    "        \n",
    "        # Check if 'g' is in the value:\n",
    "        elif 'g' in value:\n",
    "            \n",
    "            # Find the index of 'g'\n",
    "            g_index = value.find('g')\n",
    "            \n",
    "            # Strip the remainder and whitespace\n",
    "            converted = value[:g_index].strip()\n",
    "            \n",
    "            # Check for delimiters\n",
    "            if '-' in converted:\n",
    "                result = split_values(converted, \"-\")\n",
    "            elif '/' in converted:\n",
    "                result = split_values(converted, \"/\")\n",
    "            else:\n",
    "                result = converted\n",
    "\n",
    "            return result\n",
    "        \n",
    "        ### CALCULATE AVERAGES ###\n",
    "        elif '/' in value:\n",
    "            return split_values(value, \"/\")\n",
    "        \n",
    "        elif '-' in value:\n",
    "            return split_values(value, \"-\")\n",
    "        \n",
    "        ### OTHER CLEANING ###\n",
    "        elif ',' in value:\n",
    "            return float(value.replace(\",\", \".\"))\n",
    "        \n",
    "        # Catchall\n",
    "        else:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c55bd56-02f2-4307-ac55-786dccf148ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation(df, cat_col, num_col):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Check whether the missing data should be\n",
    "    imputed with a mean or a median.\n",
    "    \n",
    "    If the absolute skew score > 0.5, MEDIAN.\n",
    "    Else, not skewed, MEAN.\n",
    "    \n",
    "    Input:\n",
    "    df = dataframe\n",
    "    cat_col = categorical column\n",
    "    num_col = numerical column\n",
    "        \n",
    "    Output:\n",
    "        histogram\n",
    "        skewness score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the unique values from the categorical column\n",
    "    unique_values = df[cat_col].unique()\n",
    "    print(unique_values)\n",
    "    \n",
    "    # Loop through each unique value\n",
    "    for unique in unique_values:\n",
    "        \n",
    "        # If the value is 'never', set the value to `0`\n",
    "        df.loc[df[cat_col] == 'never', num_col] = 0\n",
    "        \n",
    "        # Get the non-null values per unique\n",
    "        check_condition = (df[cat_col] == unique) & (~df[num_col].isnull())\n",
    "        check_data = df.loc[check_condition, num_col]\n",
    "        \n",
    "        # Check for unique values with 'nan' values\n",
    "        check_null = df.loc[df[cat_col] == unique, num_col].value_counts(dropna=False).index\n",
    "        \n",
    "        # Skip if all values valid\n",
    "        if (np.nan not in check_null) or (len(check_data) == 0):\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            # Convert values for plotting\n",
    "            check_data = check_data.astype(float)\n",
    "            \n",
    "            # Histogram\n",
    "            check_data.hist()\n",
    "            \n",
    "            # Clean title\n",
    "            title = cat_col.replace(\"_\", \" \").title()\n",
    "            plt.title(f'{title} ({unique})')\n",
    "            plt.xlabel(f'{title} per Day')\n",
    "            plt.ylabel('Frequency')\n",
    "            \n",
    "            # Calculate the skewness\n",
    "            skew_score = skew(check_data)\n",
    "            \n",
    "            # Display the results\n",
    "            plt.show()\n",
    "            \n",
    "            # Check the skew score\n",
    "            if abs(skew_score) > 0.5: # skewed = use median\n",
    "                chosen_value = df.loc[check_condition, num_col].astype(float).median()\n",
    "                print(f'Skewness: {round(skew_score,2)}, use MEDIAN.')\n",
    "                print(f'Median value for \"{unique}\": {round(chosen_value, 2)}')\n",
    "            \n",
    "            else:\n",
    "                chosen_value = df.loc[check_condition, num_col].astype(float).mean()\n",
    "                print(f'Skewness: {round(skew_score,2)}, use MEAN.')\n",
    "                print(f'Mean value for \"{unique}\": {round(chosen_value, 2)}')\n",
    "            \n",
    "            # Round the chosen value\n",
    "            rounded_value = round(chosen_value, 2)\n",
    "            \n",
    "            # Impute the missing values\n",
    "            null_condition = (df[cat_col] == unique) & (df[num_col].isnull())\n",
    "            df.loc[null_condition, num_col] = rounded_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e26841f-88a4-4ce0-81d1-70e0fac7c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fruit_gram(value):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    - Convert gram-value to number of fruits.\n",
    "    - To be used with `apply()`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a standard serve\n",
    "    std_serve = 150\n",
    "    \n",
    "    # Values which do not need cleaning\n",
    "    try:\n",
    "        # Look for values greater than 5\n",
    "        if float(value) > 5:\n",
    "\n",
    "            # Convert to number of fruits\n",
    "            num_fruits = round(int(value)/std_serve, 2)\n",
    "\n",
    "            return num_fruits\n",
    "        \n",
    "        # Catchall\n",
    "        else:\n",
    "            return float(value)\n",
    "    \n",
    "    except:\n",
    "        # Look for 'gramme'\n",
    "        if ' gramme' in value:\n",
    "            converted = value.replace(\" gramme\", \"\").strip()\n",
    "\n",
    "            # Convert to number of fruits\n",
    "            num_fruits = round(int(converted)/std_serve, 2)\n",
    "\n",
    "            return num_fruits\n",
    "\n",
    "        # Catchall\n",
    "        else:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13a2dc-f5f4-4354-a4dd-4a759fa96277",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23ca8ed-c72d-4b74-aa64-9293e5350935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_option(new_width, new_height, input_dir_path, output_dir_name):\n",
    "    \"\"\"\n",
    "    PURPOSE: Resize a set of images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input files\n",
    "    input_files = os.listdir(input_dir_path)\n",
    "    \n",
    "    # Loop through each file\n",
    "    for image_name in tqdm(input_files, desc=f\"Resizing spectrograms ({output_dir_name})\"):\n",
    "        \n",
    "        # Only read .png files\n",
    "        if image_name.endswith(\".png\"):\n",
    "            \n",
    "            # Open the image file\n",
    "            img = pil_Image.open(input_dir_path + image_name)\n",
    "\n",
    "            # Resize\n",
    "            resized = img.resize((new_width, new_height))\n",
    "\n",
    "            # Create a new figure\n",
    "            plt.figure(figsize=(new_width / 100, new_height / 100))\n",
    "\n",
    "            # Plot the resized image\n",
    "            plt.imshow(resized)\n",
    "\n",
    "            # Define the filename\n",
    "            filename = image_name.split(\".\")[0]\n",
    "\n",
    "            # Remove labels and border\n",
    "            plt.tight_layout()\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Export image\n",
    "            plt.savefig(\n",
    "                f'../resources/spectrograms/{output_dir_name}/{filename}.png',\n",
    "                bbox_inches = 'tight',\n",
    "                pad_inches = 0\n",
    "            )\n",
    "\n",
    "            # Close the figure to avoid runtime warning\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36886488-7f2a-40bf-9589-32b3f46afd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_to_csv(original_path, output_name):\n",
    "    \"\"\"\n",
    "    PURPOSE: Convert spectrograms to RGBA CSV files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the resized files\n",
    "    resized_files = os.listdir(original_path)\n",
    "    \n",
    "    # Initialise lists to hold the dictionaries\n",
    "    spectro_list = []\n",
    "    id_list = []\n",
    "    r_list = []\n",
    "    g_list = []\n",
    "    b_list = []\n",
    "    a_list = []\n",
    "    \n",
    "    ### PARSE TO LISTS ###\n",
    "    # Loop through each image\n",
    "    for resized_image in resized_files:\n",
    "        \n",
    "        # Only read .png files\n",
    "        if resized_image.endswith(\".png\"):\n",
    "            \n",
    "            # Initialise a dictionary to hold the pixels\n",
    "            spectro_dict = dict()\n",
    "\n",
    "            # Open the image file\n",
    "            img = pil_Image.open(original_path + resized_image)\n",
    "\n",
    "            # Convert image to array format\n",
    "            img_array = img_to_array(img)\n",
    "\n",
    "            # Add image attributes and array to dictionary\n",
    "            spectro_dict['id'] = resized_image.split(\".\")[0]\n",
    "            spectro_dict['format'] = img.format\n",
    "            spectro_dict['mode'] = img.mode\n",
    "            spectro_dict['width_px'] = img.width\n",
    "            spectro_dict['height_px'] = img.height\n",
    "\n",
    "            # Append the dictionary to the list\n",
    "            spectro_list.append(spectro_dict)\n",
    "\n",
    "            # Append the 'id' for use as an index later\n",
    "            id_list.append(resized_image.split(\".\")[0])\n",
    "\n",
    "            # Populate the RGBA lists\n",
    "            r_list.append(img_array[:, :, 0].flatten().astype(int))\n",
    "            g_list.append(img_array[:, :, 1].flatten().astype(int))\n",
    "            b_list.append(img_array[:, :, 2].flatten().astype(int))\n",
    "            a_list.append(img_array[:, :, 3].flatten().astype(int))\n",
    "\n",
    "    # Create a list of RGBA lists\n",
    "    rgba_list = [r_list, g_list, b_list, a_list]\n",
    "\n",
    "    \n",
    "    ### SPECTROGRAM METADATA ###  \n",
    "    # Convert the dictionary to a JSON\n",
    "    json_data = json.dumps(spectro_list, indent=2)\n",
    "\n",
    "    # Specify the file path within your repository\n",
    "    file_path = '../voice_app/assets/spec_metadata.json'\n",
    "\n",
    "    # Export JSON data to a file\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "        \n",
    "    \n",
    "    ### EXPORT TO CSV ###\n",
    "    # Colour reference list\n",
    "    colours = ['r', 'g', 'b', 'a']\n",
    "    \n",
    "    # Loop through each file in the RGBA list\n",
    "    for idx, colour_list in tqdm(\n",
    "        enumerate(rgba_list),\n",
    "        desc = f\"Exporting as CSV ({output_name})\"):\n",
    "        \n",
    "        # Create a dataframe for each channel\n",
    "        df = pd.DataFrame(colour_list)\n",
    "        \n",
    "        # Use the 'id' as the index\n",
    "        df.index = id_list\n",
    "        \n",
    "        # Export to CSV\n",
    "        df.transpose().to_csv(\n",
    "            f'../resources/clean_data/spectrogram/{output_name}{colours[idx]}val.csv',\n",
    "            encoding = 'utf8',\n",
    "            index = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf0d4b-2a20-4b4a-a64e-5946a82b3785",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68e32472-eaae-4c4b-a868-2e0d1be1e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_unique(df, max_value, columns_to_limit):\n",
    "    \"\"\"\n",
    "    Purpose of the function is to limit the number of unique values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop through each column\n",
    "    for col in columns_to_limit:\n",
    "        # Get the value counts of the column\n",
    "        total_counts = df[col].value_counts()\n",
    "        \n",
    "        # Get the top values to retain, not including \"Other\"\n",
    "        top_counts = total_counts[:max_value-1]\n",
    "        \n",
    "        # Define the cutoff\n",
    "        cutoff_value = top_counts.iloc[-1]\n",
    "        \n",
    "        # Create a list of values to replace\n",
    "        replace_values = total_counts.loc[total_counts.values < cutoff_value].index\n",
    "        \n",
    "        # Replace in dataframe\n",
    "        for value in replace_values:\n",
    "            df[col] = df[col].replace(value, \"other\")\n",
    "        \n",
    "        # Check to make sure binning was successful\n",
    "        print(df[col].value_counts())\n",
    "        print(f'Number of unique values: {df[col].nunique()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b66b7f80-8cf7-4834-9ad0-4dfe66cea621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_binary(value):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    - Encode 'healthy' to '0'\n",
    "    - All other options to '1'\n",
    "    \"\"\"\n",
    "    \n",
    "    if value == 'healthy':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb8beeb-167e-4b96-aa0c-75bef0b266ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_multi(value):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    - Encode 'healthy' to '0'\n",
    "    - Encode 'reflux laryngitis' to '1'\n",
    "    - Encode 'hypokinetic dysphonia' to '2'\n",
    "    - Encode 'hyperkinetic dysphonia' to '3'\n",
    "    \"\"\"\n",
    "    \n",
    "    if value == 'healthy':\n",
    "        return 0\n",
    "    elif value == 'reflux laryngitis':\n",
    "        return 1\n",
    "    elif value == 'hypokinetic dysphonia':\n",
    "        return 2\n",
    "    elif value == 'hyperkinetic dysphonia':\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cd42e2-1ad5-412b-819e-e28435e82568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    nn_model = Sequential()\n",
    "    \n",
    "    # Choose activation function in hidden layers\n",
    "    activation_first_hidden = hp.Choice('activation_layer_0', activation_functions)\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(Dense(\n",
    "        units = hp.Int(\n",
    "            'units_layer_0',\n",
    "            min_value = 1,\n",
    "            max_value = max_num_neurons,\n",
    "            step = step_count),\n",
    "        activation = activation_first_hidden,\n",
    "        kernel_regularizer = reg_kernel,\n",
    "        input_dim = number_input_features\n",
    "    ))\n",
    "    \n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    num_layers = hp.Int('num_layers', 1, max_hidden_layers-1) # options: 1, 2\n",
    "    \n",
    "    for i in range(1, num_layers+1): # i-values: 1, 2 only\n",
    "        # Choose the number of neurons per layer\n",
    "        units_layer_i = hp.Int(\n",
    "            f'units_layer_{i}',\n",
    "            min_value = 1,\n",
    "            max_value = max_num_neurons,\n",
    "            step = step_count\n",
    "        )\n",
    "        \n",
    "        # Choose a different activation function for each layer\n",
    "        activation_layer_i = hp.Choice(f'activation_layer_{i}', activation_functions)\n",
    "\n",
    "        nn_model.add(Dense(\n",
    "            units = units_layer_i,\n",
    "            activation = activation_layer_i,\n",
    "            kernel_regularizer = reg_kernel\n",
    "        ))\n",
    "\n",
    "    # Add the output layer\n",
    "    nn_model.add(Dense(\n",
    "        units = output_layer_neurons,\n",
    "        activation = output_layer_activation\n",
    "    ))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(\n",
    "        loss = compile_loss,\n",
    "        optimizer = compile_opt,\n",
    "        metrics = [\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return(nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4355a1c-1525-4b9c-89df-a2e2c47596d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
